{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2MB5-_of_DI"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.polynomial.hermite import hermgauss\n",
        "from scipy import integrate, stats, special, optimize\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy.integrate import quad, dblquad\n",
        "from scipy.special import logsumexp, expit\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Numerical Integration"
      ],
      "metadata": {
        "id": "TgjlkhM-Y_aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(4309)\n",
        "X = 0.5\n",
        "\n",
        "##### 1: Creating function #####\n",
        "def binomial_logit(beta_i, pdf = norm.pdf, mu = 0, sigma = 1):\n",
        "  logit_prob = expit(beta_i * X)\n",
        "  density = pdf(beta_i, loc = mu, scale = sigma)\n",
        "  return (logit_prob * density)\n",
        "\n",
        "\n",
        "\n",
        "##### 2: True integral #####\n",
        "true_int, _ = quad(binomial_logit,\n",
        "                   -np.inf,\n",
        "                   np.inf,\n",
        "                   args = (norm.pdf, 0.5, np.sqrt(2)),\n",
        "                   epsabs = 1e-14)\n",
        "\n",
        "\n",
        "\n",
        "##### 3: Monte-Carlo integral #####\n",
        "draw_1 = np.random.normal(loc = 0.5,\n",
        "                          scale = np.sqrt(2),\n",
        "                          size = 20)\n",
        "\n",
        "draw_2 = np.random.normal(loc = 0.5,\n",
        "                          scale = np.sqrt(2),\n",
        "                          size = 400)\n",
        "\n",
        "int_1 = np.mean(expit(draw_1 * X))\n",
        "int_2 = np.mean(expit(draw_2 * X))\n",
        "\n",
        "\n",
        "\n",
        "###### 4: Gauss-Hermite integral #####\n",
        "# For k = 4\n",
        "nodes4, weights4 = hermgauss(4)\n",
        "int_gh4 = (1/np.sqrt(np.pi)) * np.sum(weights4 * expit(((np.sqrt(2) * np.sqrt(2) * nodes4) + 0.5)))\n",
        "\n",
        "# For k = 12\n",
        "nodes12, weights12 = hermgauss(12)\n",
        "int_gh12 = (1/np.sqrt(np.pi)) * np.sum(weights12 * expit(((np.sqrt(2) * np.sqrt(2) * nodes12) + 0.5)))\n",
        "\n",
        "# For k = 15\n",
        "nodes15, weights15 = hermgauss(15)\n",
        "int_gh15 = (1/np.sqrt(np.pi)) * np.sum(weights15 * expit(((np.sqrt(2) * np.sqrt(2) * nodes15) + 0.5)))\n",
        "\n",
        "\n",
        "\n",
        "##### 6: Same procedure for 2D case #####\n",
        "X1, X2 = 0.5, 1\n",
        "mu1, mu2 = 0.5, 1\n",
        "sigma1, sigma2 = 2, 1\n",
        "\n",
        "##### Creating function #####\n",
        "def binomial_logit_2d(beta1, beta2,\n",
        "                      pdf = norm.pdf,\n",
        "                      mu1 = mu1, sigma1 = sigma1,\n",
        "                      mu2 = mu2, sigma2 = sigma2,\n",
        "                      X1 = X1, X2 = X2):\n",
        "    \"\"\"\n",
        "    Returns the integrand for the 2D binomial logit probability:\n",
        "    expit(0.5*beta1 + 1*beta2) times the product of two independent normal densities.\n",
        "    \"\"\"\n",
        "    util = X1 * beta1 + X2 * beta2\n",
        "    return expit(util) * pdf(beta1, loc = mu1, scale = sigma1) * pdf(beta2, loc = mu2, scale = sigma2)\n",
        "\n",
        "##### True integral #####\n",
        "true_int_2d, err_2d = dblquad(\n",
        "                              lambda b2, b1: binomial_logit_2d(b1, b2),\n",
        "                              -np.inf, np.inf,\n",
        "                              lambda b1: -np.inf,\n",
        "                              lambda b1: np.inf,\n",
        "                              epsabs=1e-14\n",
        "                             )\n",
        "\n",
        "##### Monte-Carlo integral #####\n",
        "draws_20_b1 = np.random.normal(loc = mu1, scale = sigma1, size = 20)\n",
        "draws_20_b2 = np.random.normal(loc = mu2, scale = sigma2, size = 20)\n",
        "mc_int_20 = np.mean(expit(X1*draws_20_b1 + X2*draws_20_b2))\n",
        "\n",
        "draws_400_b1 = np.random.normal(loc=mu1, scale=sigma1, size=400)\n",
        "draws_400_b2 = np.random.normal(loc=mu2, scale=sigma2, size=400)\n",
        "mc_int_400 = np.mean(expit(X1*draws_400_b1 + X2*draws_400_b2))\n",
        "\n",
        "###### Gauss-Hermite integral #####\n",
        "k = 5\n",
        "nodes, weights = hermgauss(k)\n",
        "\n",
        "# In 1-D we use: beta = sqrt(2)*sigma*x + mu, and then the approximation is\n",
        "# I ≈ 1/√π Σ w_i * expit( (√2*σ*x_i + mu)*X )\n",
        "# In 2-D, the joint transformation gives a prefactor 1/π.\n",
        "gh_sum = 0.0\n",
        "for i in range(k):\n",
        "    for j in range(k):\n",
        "        beta1 = np.sqrt(2)*sigma1*nodes[i] + mu1\n",
        "        beta2 = np.sqrt(2)*sigma2*nodes[j] + mu2\n",
        "        gh_sum += weights[i] * weights[j] * expit(X1*beta1 + X2*beta2)\n",
        "\n",
        "gh_int_2d = gh_sum / np.pi  # because (1/√π)² = 1/π\n",
        "\n",
        "\n",
        "\n",
        "##### 7: Comparison tables #####\n",
        "# Calculating errors for each method\n",
        "error_int_1 = abs(int_1 - true_int)\n",
        "error_int_2 = abs(int_2 - true_int)\n",
        "error_int_gh4 = abs(int_gh4 - true_int)\n",
        "error_int_gh12 = abs(int_gh12 - true_int)\n",
        "error_int_gh15 = abs(int_gh15 - true_int)\n",
        "\n",
        "error_mc_int_20 = abs(mc_int_20 - true_int_2d)\n",
        "error_mc_int_400 = abs(mc_int_400 - true_int_2d)\n",
        "error_gh_int_2d = abs(gh_int_2d - true_int_2d)\n",
        "\n",
        "# Creating 1D table\n",
        "data_1d = {\n",
        "    \"Method\": [\"True integral\", \"Monte Carlo (20)\", \"Monte Carlo (400)\",\n",
        "               \"Gauss-Hermite (4)\", \"Gauss-Hermite (12)\", \"Gauss-Hermite (15)\"],\n",
        "    \"Approximation value\": [true_int, int_1, int_2, int_gh4, int_gh12, int_gh15],\n",
        "    \"True value\": [true_int] * 6,\n",
        "    \"Error\": [0, error_int_1, error_int_2, error_int_gh4, error_int_gh12, error_int_gh15],\n",
        "    \"Number of points used\": [np.nan, 20, 400, 4, 12, 15]\n",
        "}\n",
        "\n",
        "df_1d = pd.DataFrame(data_1d)\n",
        "\n",
        "# Creating 2D table\n",
        "data_2d = {\n",
        "    \"Method\": [\"True integral\", \"Monte Carlo (20)\", \"Monte Carlo (400)\",\n",
        "               \"Gauss-Hermite (5)\"],\n",
        "    \"Approximation value\": [true_int_2d, mc_int_20, mc_int_400, gh_int_2d],\n",
        "    \"True value\": [true_int_2d] * 4,\n",
        "    \"Error\": [0, error_mc_int_20, error_mc_int_400, error_gh_int_2d],\n",
        "    \"Number of points used\": [np.nan, 20*2, 400*2, 25]  # 20 pairs = 40, 400 pairs = 800, 5x5 for 2D Gauss-Hermite\n",
        "}\n",
        "\n",
        "df_2d = pd.DataFrame(data_2d)"
      ],
      "metadata": {
        "id": "3BGsq6aXs07v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1d"
      ],
      "metadata": {
        "id": "z4UEqjNDA-wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2d"
      ],
      "metadata": {
        "id": "lyD8dk-ZBACC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Maximum Likelihood"
      ],
      "metadata": {
        "id": "pqsf7QeckD4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('pset1_data.csv').drop(columns = 'Unnamed: 0')\n",
        "\n",
        "# Input data on outside option explicitly\n",
        "outside_option = (\n",
        "                  data.groupby('individual_id_i')\n",
        "                  .apply(lambda x: pd.Series({\n",
        "                      'apt_id_j': 20, #index for outside option\n",
        "                      'logsqfeet_j': 0,\n",
        "                      'bath_j': 0,\n",
        "                      'outdoor_j': 0,\n",
        "                      'y_ij': 1 if x['y_ij'].sum() == 0 else 0,\n",
        "                      'family_size_i': x['family_size_i'].iloc[0]\n",
        "                  }, index = ['apt_id_j', 'logsqfeet_j',\n",
        "                              'bath_j', 'outdoor_j',\n",
        "                              'y_ij', 'family_size_i']))\n",
        "                  .reset_index()\n",
        "                 )\n",
        "\n",
        "# Merge datasets\n",
        "full_data = pd.concat([\n",
        "                        data[['individual_id_i', 'apt_id_j',\n",
        "                              'family_size_i', 'logsqfeet_j',\n",
        "                              'bath_j', 'outdoor_j', 'y_ij']],\n",
        "                        outside_option\n",
        "                       ],\n",
        "                       ignore_index = True)\n",
        "\n",
        "# Create feature associated with β3 and the intercept\n",
        "full_data['bath_family'] = full_data['bath_j'] * full_data['family_size_i']\n",
        "full_data['intercept'] = 1\n",
        "\n",
        "# Verify exactly 1 choice per individual\n",
        "assert (full_data.groupby('individual_id_i')['y_ij'].sum() == 1).all()\n",
        "\n",
        "\n",
        "\n",
        "##### 1: Plain logit #####\n",
        "# Computing the negative log likelihood using the max trick in Part 1\n",
        "def neg_log_lik(params):\n",
        "\n",
        "    # Define predictors used in the utility function\n",
        "    feature_columns = ['intercept', 'logsqfeet_j',\n",
        "                       'bath_j', 'bath_family', 'outdoor_j']\n",
        "\n",
        "    # Compute utilities for all alternatives\n",
        "    utilities = full_data[feature_columns].dot(params)\n",
        "\n",
        "    # Compute per-individual maximum utility (for numerical stability)\n",
        "    group_max = utilities.groupby(full_data['individual_id_i']).transform('max')\n",
        "\n",
        "    # Compute exponentiated utilities relative to the group max\n",
        "    exp_term = np.exp(utilities - group_max)\n",
        "\n",
        "    # Compute the sum of exponentiated utilities for each individual\n",
        "    sum_exp = exp_term.groupby(full_data['individual_id_i']).transform('sum')\n",
        "\n",
        "    # Compute log-sum-exp for each observation (each group has the same value)\n",
        "    logsumexp_series = group_max + np.log(sum_exp)\n",
        "\n",
        "    # Identify the index of the chosen alternative per individual using the indicator variable\n",
        "    chosen_idx = full_data.groupby('individual_id_i')['y_ij'].idxmax()\n",
        "\n",
        "    # Extract the chosen utility and corresponding log-sum-exp for each individual\n",
        "    chosen_utilities = utilities.loc[chosen_idx]\n",
        "    group_logsumexp = logsumexp_series.loc[chosen_idx]\n",
        "\n",
        "    # Sum over individuals and return the negative log-likelihood for minimization\n",
        "    return -(chosen_utilities - group_logsumexp).sum()\n",
        "\n",
        "# Optimization\n",
        "result = minimize(\n",
        "                  neg_log_lik,\n",
        "                  x0 = np.zeros(5),\n",
        "                  method = 'BFGS',\n",
        "                  jac = False,\n",
        "                  hess = True,\n",
        "                  options = {\n",
        "                      'disp': False,\n",
        "                      'maxiter': 1000,\n",
        "                      'gtol': 1e-6,\n",
        "                      'ftol': 1e-6,\n",
        "                  }\n",
        "                  )\n",
        "\n",
        "\n",
        "\n",
        "##### 2: Mixed logit via Gauss-Hermite\n",
        "# Pre-calculate chosen indices per individual from full_data:\n",
        "chosen_idx = full_data.groupby('individual_id_i')['y_ij'].idxmax()\n",
        "groups = full_data['individual_id_i']\n",
        "\n",
        "def neg_log_lik_mixed(params, gh_nodes, gh_weights):\n",
        "    beta0, beta1, beta2, beta3, gamma, sigma = params\n",
        "    X = full_data[['intercept', 'logsqfeet_j', 'bath_j', 'bath_family']].values\n",
        "    outdoor = full_data['outdoor_j'].values\n",
        "\n",
        "    # Pre-calculate the fixed (non-random) component\n",
        "    fixed_util = X.dot(np.array([beta0, beta1, beta2, beta3]))\n",
        "\n",
        "    unique_ids = full_data['individual_id_i'].unique()\n",
        "    likelihood = pd.Series(0.0, index = unique_ids)\n",
        "\n",
        "    # Loop over the Gauss-Hermite nodes and weights\n",
        "    for x, w in zip(gh_nodes, gh_weights):\n",
        "        adjust = gamma + sigma * np.sqrt(2) * x\n",
        "        util = fixed_util + adjust * outdoor\n",
        "\n",
        "        # For numerical stability compute group maximum utility per individual:\n",
        "        util_series = pd.Series(util, index = full_data.index)\n",
        "        group_max = util_series.groupby(groups).transform('max')\n",
        "        exp_util = np.exp(util - group_max)\n",
        "        # Sum exp utilities by individual (denom of logit probability)\n",
        "        denom = exp_util.groupby(groups).transform('sum')\n",
        "\n",
        "        # For each individual, get the probability of the chosen alternative.\n",
        "        p_chosen = np.exp(util_series.loc[chosen_idx] - group_max.loc[chosen_idx]) / denom.loc[chosen_idx]\n",
        "\n",
        "        likelihood += (w / np.sqrt(np.pi)) * p_chosen\n",
        "\n",
        "    # Avoid numerical issues by bounding from below\n",
        "    likelihood = likelihood.clip(lower = 1e-300)\n",
        "    # Negative log likelihood: sum over individuals\n",
        "    nll = -np.log(likelihood).sum()\n",
        "    return nll\n",
        "\n",
        "# Set up Gauss-Hermite quadrature with a chosen number of nodes (say, 20 nodes)\n",
        "n_nodes = 5\n",
        "gh_nodes, gh_weights = np.polynomial.hermite.hermgauss(n_nodes)\n",
        "\n",
        "result_mixed = minimize(\n",
        "                        neg_log_lik_mixed,\n",
        "                        x0 = np.zeros(6),\n",
        "                        args = (gh_nodes, gh_weights),\n",
        "                        method = 'BFGS',\n",
        "                        jac = False,\n",
        "                        hess = True,\n",
        "                        options = {\n",
        "                                'disp': False,\n",
        "                                'maxiter': 1000,\n",
        "                                'gtol': 1e-6,\n",
        "                                'ftol': 1e-6,\n",
        "                        }\n",
        "                        )\n",
        "\n",
        "\n",
        "\n",
        "##### 3: Standard errors from mixed logit #####\n",
        "# Calculate the Hessian matrix (information matrix)\n",
        "hessian_plain = result.hess_inv\n",
        "\n",
        "# Compute the standard errors (square roots of diagonal elements of the covariance matrix)\n",
        "std_errors_plain = np.sqrt(np.diag(hessian_plain))\n",
        "\n",
        "# Calculate the Hessian matrix (information matrix)\n",
        "hessian_mixed = result_mixed.hess_inv\n",
        "\n",
        "# Compute the standard errors (square roots of diagonal elements of the covariance matrix)\n",
        "std_errors_mixed = np.sqrt(np.diag(hessian_mixed))\n",
        "\n",
        "# Report results:\n",
        "print(\"\\nPlain logit\")\n",
        "print(f\"Final Estimates:\\n{pd.Series(result.x, index = ['β0', 'β1','β2', 'β3', 'γ'])}\")\n",
        "print(f\"\\nStandard errors for plain logit estimates:\\n{pd.Series(std_errors_plain, index = ['β0', 'β1', 'β2', 'β3', 'γ'])}\")\n",
        "\n",
        "print(\"\\nMixed Logit via Gauss-Hermite Integration\")\n",
        "print(f\"Final Estimates:\\n{pd.Series(result_mixed.x, index = ['β0', 'β1', 'β2', 'β3', 'γ', 'σ'])}\")\n",
        "print(f\"\\nStandard errors for Gauss Hermite estimates:\\n{pd.Series(std_errors_mixed, index = ['β0', 'β1', 'β2', 'β3', 'γ', 'σ'])}\")"
      ],
      "metadata": {
        "id": "mY1z4uK6sXyh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}